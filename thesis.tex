\PassOptionsToPackage{table}{xcolor}

\documentclass[12pt]{ucsddissertation}
% mathptmx is a Times Roman look-alike (don't use the times package)
% It isn't clear if Times is required. The OGS manual lists several
% "standard fonts" but never says they need to be used.
\usepackage{mathptmx}
\usepackage[NoDate]{currvita}
\usepackage{array}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{ragged2e}
\usepackage{microtype}
\usepackage[breaklinks=true,pdfborder={0 0 0}]{hyperref}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{enumitem}
\usepackage{arydshln}
\AtBeginDocument{%
	\settowidth\cvlabelwidth{\cvlabelfont 0000--0000}%
}

%commands

\newcommand{\sword}{SwordBox}
\newcommand{\thesistitle}{Enze Liu's Thesis Title \\ Blah blah}

\usepackage{multirow}
\usepackage{tikz}
\newcommand*\nullcirc[1][1ex]{\tikz\draw (0,0) circle (3pt);} 
\newcommand*\halfcirc[1][1ex]{%
  \begin{tikzpicture}
  \draw[fill] (0,0)-- (90:3pt) arc (90:270:3pt) -- cycle ;
  \draw (0,0) circle (3pt);
  \end{tikzpicture}}
\newcommand*\fullcirc[1][1ex]{\tikz\fill (0,0) circle (3pt);} 

% OGS recommends increasing the margins slightly.
\increasemargins{.1in}

% These are just for testing/examples, delete them
\usepackage{trace}
%\usepackage{showframe} % This package was just to see page margins
% \usepackage[table]{xcolor}

\usepackage[english]{babel}
\usepackage{blindtext}
\input{annotations}
\overfullrule=5pt
% ---

% Required information
\title{\thesistitle}
\author{Enze Liu}
\degree{Computer Science}{Doctor of Philosopy}
% Each member of the committee should be listed as Professor Foo Bar.
% If Professor is not the correct title for one, then titles should be
% omitted entirely.
\cochair{Professor Stefan Savage}
\cochair{Professor Geoffrey M. Voelker}
% Your committee members (other than the chairs) must be in alphabetical order
\committee{Professor KC Claffy}
\committee{Doctor David D Clark}
\committee{Professor Terrance August}
\degreeyear{2025}

% Start the document
\begin{document}
% Begin with frontmatter and so forth
\frontmatter
\maketitle
\makecopyright
\makesignature
% Optional
\begin{dedication}
\setsinglespacing
\raggedright % It would be better to use \RaggedRight from ragged2e
\parindent0pt\parskip\baselineskip

\textit{To Mom and Dad, who raise me as a human being; to Stefan and Geoff, who raise me as a researcher; and to everyone who believes in and invests in me --- you made me who I am today.}


% In recognition of reading this manual before beginning to format the
% doctoral dissertation or master's thesis; for following the
% instructions written herein; for consulting with OGS Academic Affairs
% Advisers; and for not relying on other completed manuscripts, this
% manual is dedicated to all graduate students about to complete the
% doctoral dissertation or master's thesis.

% In recognition that this is my one chance to use whichever
% justification, spacing, writing style, text size, and/or textfont that
% I want to while still keeping my headings and margins consistent.
\end{dedication}
% Optional
\begin{epigraph}
\vskip0pt plus.5fil
\setsinglespacing
\begin{flushright}
``It takes a village to raise a child.''\\
\vskip\baselineskip
% -Voltaire \textit{Candide}\par
- African Proverb
\end{flushright}

\vfil
\begin{center}

% \noindent ``It must be considered that there is nothing more difficult to carry out, nor more
% doubtful of success, nor more dangerous to handle, than to initiate a new order of things.''

\vskip\baselineskip
% \hskip0pt plus1fil -Niccolo Machiavelli \textit{The Prince}\hskip0pt plus4fil\null

\end{center}
\vfil
``My ears had heard of you but now my eyes have seen you.''\\

\vskip\baselineskip
- Job 42:5 (\textit{NIV})

\vfil
\end{epigraph}

% Next comes the table of contents, list of figures, list of tables,
% etc. If you have code listings, you can use \listoflistings (or
% \lstlistoflistings) to have it be produced here as well. Same with
% \listofalgorithms.
\tableofcontents
\listoffigures
\listoftables

% Preface
% \begin{preface}
% \todo{Write a preface - take a look at some examples because it seems rather free form}
% Almost nothing is said in the manual about the preface. There is no
% indication about how it is to be typeset. Given that, one is forced to
% simply typeset it and hope it is accepted. It is, however, optional
% and may be omitted.
% \end{preface}

% Your fancy acks here. Keep in mind you need to ack each paper you
% use. See the examples here. In addition, each chapter ack needs to
% be repeated at the end of the relevant chapter.
\begin{acknowledgements}
Graduate school is full of challenges and uncertainty. In retrospect, I would have not been able to finish it without the help and support from the amazing people that surround me, both past and present.
% Chapter~\ref{chap:swordbox} is a partial reprint of work submitted to multiple USENIX and ACM
% conferences under the title "SwordBox: Accelerating Shared Access in RDMA-based Disaggregated
% Memory. Stewart Grant, Alex C. Snoeren. This dissertations author was the primary investigator and
% author of this paper.
% %%
% Chapter~\ref{chap:rcuckoo} is a partial reprint of work submitted to multiple USENIX conferences
% under the title "Cuckoo for Clients: Disaggregated Cuckoo Hashing. Stewart Grant, Alex C. Snoeren.
% This dissertations author was the primary investigator and author of this paper.

First and foremost, I would like to thank my wonderful advisors, Stefan and Geoff. A pair of advisors that are unmatchable. 

In addition to my advisors, I also had the great opportunity to work with a variety of amazing faculty from CSE: Aaron Schulman, Deian Stefan, Kristen Vacarro, and Imani Munyaka. Our other faculty has helped me during various occasions: Deepak Kumar, Earlence Fernandes, Alex C. Snoeren, and Amy Ousterhout.


I thank my committee members, who have helped me look at things from different perspectives.  Dave and KC have patiently listened to my poorly prepared presentations and gave me valuable feedback. Terrance helped me understand the economic part of cybercrime.

I also thank all the residents of 3140, many of whom have played important role in my life and deeply shaped who I am. In particular, I thank Stewart Grant, with whom I share countless fond memories. Stew is my faithful guide to North American culture and has helped me become a much better person in my aspects. I also thank Miro Haller, my swiss counterpart. Miro and I have
 taken many classes together because of our shared hobbies, and I would not have been who I am without all of those hobbies. Audrey kindly took me under her wings when I did not know what I was doing. Gautam mentored me through the early days. Ariana demonstrated the definition of leadership. Anil Yelam introduced me to tennis, which became one of my favorite sports. Keegan Ryan blah...

I thank other students in Sysnet, many of them have also had big impact on me. Nishant acts like our old brother and is a source of infinite wisdom. I have also learned a host of things by interacting with others (names).

I also had the great opportunity to befriend amazing people from CSE and other departments. Lu Sun, who redefines the meaning of hardworking, has taught me many important lessons of friendship. I am truly grateful to have her as a close friend. I also extend my gratitude to Hengyuan Zhang and his partner Wenqing Tang, for being great travel partners and tolerating my temper every now and then. I would not have stayed sane without the fantastic times I spent with the tennis group (big shout out to Shuheng Li, Alex Yen, Daniel Spokoyny, Nikolai Vogler, Chris Priebe, Nithin Raghavan), the running club (kudos to head coach Zac Blanco), and the recently formed surfing group, and in the dance studio (which would not have been possible without Jennifer Chien).

I would not have made it had my life not extend beyond the boarder of UCSD. I thank friends like Camille Rubel, Ally Nisenoff, and friends I made from or through living water bible church. My guitar teach Warner Iveries.

I also thank other people who have helped me professionally, including Liz Izhikevich from UCLA, Brad Chen, Weihaw Chuang, Kurt Thomas, and Sarah Meiklejohn from Google, my past and present collaborators from UChicago, Blase Ur (my masters' advisor), David Cash, Ben Zhao, Grant Ho, the wonderful UCSD students I collaborated with (names Rama), and my collaborators from all over the world (names).

I thank CSE staff members (Tina, Tierra, and Valerie) and visit day volunteers (names) for devoting their time to make our department a better place. I also thank Jennifer Folkestad and Cindy Moore.

My supportive and understanding roommates: Yagen Jin, Mike Chen, Cheng Li, and Laura O. 

I also grateful for many of friends who have stayed with me for years if not decades.

Anhua Chen and Yuancheng Zhu kept sane during those insane days.

Finally, I thank my parents, especially my mom, for their love and unwavering support.

Finally, I would like to thank you for reading this far and perhaps reading the rest of the dissertation. I hope you can learn something useful and share it with others. To you all, and thy happy children of tomorrow, I send my greetings.
\todo{double check I thanked everyone}


\end{acknowledgements}

% Stupid vita goes next
\begin{vita}
\noindent
\begin{cv}{}
\begin{cvlist}{}
\item[2012-2016] Bachelor of Science, Software Engineering, Beijing Institute of Technology
\item[2016-2018] Master of Science, Computer Science, University of Chicago
\item[2019-2025] Doctor of Philosophy, Computer Science, University of California San Diego

\end{cvlist}
\end{cv}

% This puts in the PUBLICATIONS header. Note that it appears inside
% the vita environment. It is optional.
\publications


\noindent \textbf{Enze Liu}, Elisa Luo, Shawn Shan, Geoffrey M. Voelker, Ben Y. Zhao, and Stefan Savage, ``Somesite I Used To Crawl: Awareness, Agency and Efficacy in Protecting Content Creators From AI Crawlers,'' in \textit{Proceedings of the ACM Internet Measurement Conference (IMC)}, 2025.\\

\noindent Alex Bellon, Miro Haller, Andrey Labunets, \textbf{Enze Liu}, and Stefan Savage, ``An Empirical Analysis on the Use and Reporting of National Security Letters,'' in \textit{Proceedings of the ACM Symposium on Computer Science and Law (CSLAW)}, 2025.\\


\noindent \textbf{Enze Liu}, George Kappos, Eric Mugnier, Luca Invernizzi, Stefan Savage, David Tao, Kurt Thomas, Geoffrey M. Voelker, and Sarah Meiklejohn, ``Give and Take: An End-To-End Investigation of Giveaway Scam Conversion Rates,'' in \textit{Proceedings of the ACM Internet Measurement Conference (IMC)}, 2024.\\

\noindent Lu Sun, Hengyuan Zhang, \textbf{Enze Liu}, Mingyang Liu, and Kristen Vaccaro, ``NewsGuesser: Using Curiosity to Reduce Selective Exposure,'' \textit{Proceedings of the ACM on Human-Computer Interaction (CSCW)}, 2024.\\

\noindent Sumanth Rao, \textbf{Enze Liu}, Grant Ho, Geoffrey M. Voelker, and Stefan Savage, ``Unfiltered: Measuring Cloud-based Email Filtering Bypasses,'' in \textit{Proceedings of the ACM Web Conference (WWW)}, 2024.\\

\noindent \textbf{Enze Liu}, Gautam Akiwate, Mattijs Jonker, Ariana Mirian, Grant Ho, Geoffrey M. Voelker, and Stefan Savage, ``Forward Pass: On the Security Implications of Email Forwarding Mechanism and Policy,'' in \textit{Proceedings of the IEEE European Symposium on Security and Privacy} (EuroS\&P), IEEE, 2023.\\

\noindent \textbf{Enze Liu}, Lu Sun, Alex Bellon, Grant Ho, Geoffrey M. Voelker, Stefan Savage, and Imani N.S. Munyaka, ``Understanding the Viability of Gmail's Origin Indicator for Identifying the Sender,'' in \textit{Proceedings of the Symposium on Usable Privacy and Security (SOUPS)}, 2023.\\

\noindent \textbf{Enze Liu}, Sumanth Rao, Sam Havron, Grant Ho, Stefan Savage, Geoffrey M. Voelker, and Damon McCoy, ``No Privacy Among Spies: Assessing the Functionality and Insecurity of Consumer Android Spyware Apps,'' \textit{Proceedings on Privacy Enhancing Technologies (PoPETS)}, 2023.\\

\noindent Anil Yelam, Stewart Grant, \textbf{Enze Liu}, Radhika Niranjan Mysore, Marcos K Aguilera, Amy Ousterhout, and Alex C. Snoeren, ``Limited Access: The Truth Behind Far Memory,'' in \textit{Proceedings of the
Workshop on Resource Disaggregation and Serverless (WORDS)}, 2023.\\

\noindent Audrey Randall, \textbf{Enze Liu}, Ramakrishna Padmanabhan, Gautam Akiwate, Geoffrey M. Voelker, Stefan Savage, and Aaron Schulman, ``Home is Where the Hijacking is: Understanding DNS Interception by Residential Routers,'' in \textit{Proceedings of the ACM Internet Measurement Conference (IMC)}, 2021.\\

\noindent \textbf{Enze Liu}, Gautam Akiwate, Mattijs Jonker, Ariana Mirian, Stefan Savage, and Geoffrey M. Voelker, ``Who's Got Your Mail? Characterizing Mail Service Provider Usage,'' in \textit{Proceedings of the ACM Internet Measurement Conference (IMC)}, 2021.\\

\noindent Audrey Randall, \textbf{Enze Liu}, Gautam Akiwate, Ramakrishna Padmanabhan, Geoffrey M. Voelker, Stefan Savage, and Aaron Schzulman, ``Trufflehunter: Cache Snooping Rare Domains at Large Public DNS Resolvers,'' in \textit{Proceedings of the ACM Internet Measurement Conference (IMC)}, 2020.\\ 

\noindent \textbf{Enze Liu}, Amanda Nakanishi, Maximilian Golla, David Cash, and Blase Ur, ``Reasoning Analytically about Password-Cracking Software,'' in \textit{Proceedings of the IEEE Symposium on Security and Privacy (S\&P)}, IEEE, 2019.\\

% \noindent Deepak Bansal, Gerald DeGrace, Rishabh Tewari, Michal Zygmunt, and James Grantham, Silvano
% Gai, Mario Baldi, Krishna Doddapaneni, Arun Selvarajan, Arunkumar Arumugam, Balakrishnan Raman,
% Avijit Gupta, Sachin Jain, Deven Jagasia, Evan Langlais, Pranjal Srivastava, Rishiraj Hazarika,
% Neeraj Motwani, Soumya Tiwari, Stewart Grant, Ranveer Chandra, and Srikanth Kandula . 2023.
% Disaggregating Stateful Network Functions. In proceedings of 20th USENIX Symposium on Networked
% Systems Design and Implementation (NSDI '23).  Usenix Association, Boston MA, USA, April 2018, 1469--1487. \\

% \noindent Stewart Grant, Anil Yelam, Maxwell Bland, and Alex C. Snoeren. 2020. SmartNIC Performance
% Isolation with FairNIC: Programmable Networking for the Cloud. In Proceedings of the Annual
% conference of the ACM Special Interest Group on Data Communication on the applications,
% technologies, architectures, and protocols for computer communication (SIGCOMM '20). Association for
% Computing Machinery, Virtual Event, August 2020, 681–693.\\

% \noindent Stewart Grant, Hendrik Cech, and Ivan Beschastnikh. 2018. Inferring and asserting
% distributed system invariants. In Proceedings of the 40th International Conference on Software
% Engineering (ICSE '18). Association for Computing Machinery, Gothenberg, Sweden, July 2018, 1149–1159.\\


% This puts in the FIELDS OF STUDY. Also inside vita and also
% optional.
% \fieldsofstudy
% \noindent Major Field: Computer Science
\end{vita}

% Put your maximum 350 word abstract here.
\begin{dissertationabstract} 
%%	
Resource disaggregation proposes a next-generation architecture for data center resources. System
components like compute, memory, storage, and accelerators are separated from one another by a fast
network and composed dynamically into virtual servers when required. This paradigm promises to
dramatically improved resource utilization, scalability, and flexibility, but introduces substantial
challenges in terms of performance and fault tolerance. Memory is among the most difficult resources
to disaggregate. CPUs currently expect DRAM to have ultra low latency, high bandwidth, and to share it's
failure domain. In particular increased latency from network round trips dramatically shifts the
performance of existing shared data structures designed for local DRAM.

In this dissertation I demonstrate the challenges of sharing disaggregated memory and show that
programmable network devices can be used to significantly improved system performance. I present two
systems: First {\sword} which utilizes a centralized programmable switch to cache data structure
state and dramatically improve key-value workload performance. Second I present a new key-value
store RCuckoo which is designed to leverage RDMA and reduce round trips when accessed by CPUs over a
network. Both systems demonstrate significant performance improvements over the existing state of
the art.

\end{dissertationabstract}

% This is where the main body of your dissertation goes!
\mainmatter

% Optional Introduction
\begin{dissertationintroduction}


\textit{What should a data center server look like?} Twenty years ago, a data center operator may
have argued for the simplicity of homogeneity over optimal performance, reasoning that carefully
picked commodity hardware at a given price point would yield the best cost-performance tradeoffs and that the
performance gains of next-generation hardware would quickly erase any benefits made by specializing
servers due to Dennard scaling.



% Since then, both Moore's law and Dennard scaling have slowed down dramatically. CPU clock speeds and
% memory density improvements have stagnated, leaving operators to fight tooth and claw to enjoy the
% efficiency gains of prior decades. The effect is that new technologies are being introduced to
% achieve scaling. CPUs are now monstrously parallel. Custom accelerators are common for specialized
% workloads like video coding and machine learning. Indeed, the modern data center is a hodgepodge of
% heterogeneous hardware (GPUs, TPUs, DPUs, SmartNICs, and
% FPGAs)~\cite{dsnf,azure-smartnic,tpu,nitro}, and various new memory offerings and tiers like
% NVMe~\cite{decible}. Today, the number of server types in a data center, conservatively, is in the
% dozens. At the time of writing, EC2 has 84 listed instance types~\cite{ec2-offer} for their
% customers to design their services. The trend is clear: in search of efficiency, data center and
% server design is increasingly heterogeneous, with servers being designed for specific applications
% and workloads.

% Resource disaggregation is a new architectural paradigm for data center resources aimed at improving
% efficiency and managing increased heterogeneity. In the disaggregated model, a server's resources do
% not monolithically reside in a 1U, 2U, or 4U server form factor. Instead, each resource (i.e.,
% compute, memory, storage) is deployed separately to a dedicated machine and interconnected via a
% fast network. Servers are composed dynamically from these resources, which enables them to be
% provisioned for their exact purpose~\cite{disandapp,infiniswap,blade-server,decible,legoos}. This
% model enables resource pooling and sharing, which in turn leads to higher
% efficiency~\cite{regions,fastswap,dsnf,aifm,supernic,ditto}.


% % DRAM in particular has quickly become a precious resource in the data center. Main memory capacity,
% % while growing has not kept up with CPU core counts. The result is that per-core memory capacity is
% % less than 10 years ago~\cite{micron-memorywall}. New memory tiers like NVMe have been introduced to
% % help alleviate memory pressure from applications, but the composition of these memory tiers, and the
% % software to manage them are still undecided.

% DRAM, in particular, has become a precious resource in data centers and is a focused target for
% resource pooling~\cite{micron-memorywall}. The benefits of pooling are clear when examining a simple
% bin packing problem. Consider two servers, each provisioned with 4GB of DRAM, and three jobs, each
% requiring 2.5GB of memory. In a monolithic design, a scheduler can only place one job per machine or
% risk swapping to disk. In a disaggregated model, the 4GB of memory could be placed in an 8GB pool,
% which could be easily subdivided into three 2.5GB partitions. In the monolithic case, the unused
% memory is stranded, while pooling reclaims stranded memory. More concretely, at data center levels,
% practitioners have to provision their servers for the sum of peak demand; when resources are pooled,
% they can be provisioned for the peak of the sum of demand, which can be significantly
% lower~\cite{ dsnf, supernic}.

% Disaggregation, in general, is only possible because of new fast networks. Commodity NICs now offer
% 400Gbps with expectations for continued growth to 800Gbps and above~\cite{cx8}. Network and memory
% bandwidths are quickly approaching the same order of magnitude. At the same time, network stacks are
% becoming lighter-weight through kernel bypass and CPU bypass technologies like DPDK~\cite{dpdk} and
% RDMA~\cite{infiniband-spec}, enabling applications to more easily take advantage of the additional
% bandwidth. Network devices themselves are becoming increasingly programmable, with multiple vendors
% offering programmable SmartNICs, DPUs, and switches~\cite{tofino2,bluefield,pensando}. These two
% trends have led to intra-rack latencies of 1-2{$\mu$}s, with the ability to inspect, cache, and
% modify packets in flight at line rate.


% Despite fast networks, memory disaggregation has remained elusive while storage, such as spinning
% disks and solid-state drives, has seen widespread disaggregation. The reason for this contrast is
% that storage device access latency is far higher than a network round trip. In the case of memory,
% the opposite is true. Memory access latency is approximately 20 times lower than a network round
% trip (50-100ns), effectively making it a separate tier of memory when placed across a network. While
% there is ongoing research demonstrating the advantages of tracking cache lines over pages for remote
% memory~\cite{kona}, the cost of fully disaggregating all memory is deemed too high~\cite{legoos}. A common
% proposal for disaggregated memory is to have CPUs with a reasonably sized cache (e.g., 4GB) of DRAM
% attached to them, along with software to manage and reduce the cost of remote
% accesses~\cite{legoos}.

% A large body of literature exists on disaggregated memory systems with a significant local cache.
% Many of these systems intervene in the virtual memory system to decrease the cost of accessing
% remote memory by employing prefetching and eviction strategies aimed at minimizing blocking remote
% accesses~\cite{infiniswap,fastswap,leap,hydra}. Similarly, object-based disaggregated systems
% utilize remotable objects with per-object tracking to mitigate the frequency of remote
% accesses~\cite{aifm,carbink}. Additionally, compiler-based systems leverage static and dynamic
% analysis to identify large, small, and hot objects for cache optimization~\cite{mira,trackfm}. These
% systems primarily focus on analyzing memory access patterns and reducing the number of remote
% accesses with decreasing volumes of local cache. However, they often overlook a critical aspect of
% memory access: sharing. When memory is shared, access patterns alone are insufficient to minimize
% round trips. Some degree of coherence must be maintained between remote caches, which is the primary
% focus of this dissertation.


% % %%
% % Sharing data between multiple remote machines, where coherence between remote cores is required,
% % leads to abysmal performance degradation (300x tail latencies in some cases)
% % (Chapter~\ref{chap:swordbox}). Further complicating the design of shared data structures is that
% % memory, once in a single fault domain, can fail, ripping a hole directly into a process's address
% % space~\cite{amanda-hotnets,hydra,legoos}.

% %%you have to add in a paragraph on current successful disaggregated systems

% At its core, the challenge of sharing in disaggregated memory is serialization. When a data
% structure is shared by multiple accessors, some mechanism must ensure the consistency of the data
% structure. In a monolithic system, this is often achieved with locks or atomic operations. Across
% machines, serialization is typically accomplished by either a centralized sequencer or a distributed
% protocol. As a point of comparison, consider the differences between serialization in a traditional
% RPC system and disaggregated memory. In an RPC system, requests arrive on a NIC and are delivered to
% one or more CPU cores for processing~\cite{cuckoo-improvements,pilaf,herd,memc3,memcached}. If all
% requests are routed through a single CPU core, it implicitly serializes operations by enqueueing the
% RPC requests and servicing them one at a time. If the RPC service has multiple cores, any shared
% structure can be protected via a lock or other synchronization mechanism in the RPC server's local
% memory. In contrast, in a disaggregated system, no such server-side CPU exists. Or, if one does, it
% is assumed to be low power and incapable of handling significant traffic. In the absence of such a
% CPU, clients must enforce serialization amongst themselves.

% In the absence of a serialization mechanism close to memory, clients must rely on the only mechanism
% available in commodity systems: RDMA atomics. Throughout this dissertation, RDMA atomics will be
% used as the backbone of all shared disaggregated data structures. They take the form of two
% operations: compare-and-swap (CAS) and fetch-and-add (FAA), which execute with the same semantics as
% their local counterparts but on the memory of a remote machine. While their semantics remain the
% same, their performance is dramatically different. As noted before, the cost of executing a remote
% operation is 20 times that of local memory. This latency inflates the size of critical sections
% built with remote locks and leads to stale caches and poor performance for optimistic data
% structures under contention~\cite{clover,sherman,fusee,race,rolex}.

% \begin{center}
% \textit{Data structures can be optimized for disaggregated memory by leveraging network programmability.} \\
% \end{center}

% This thesis statement is the core of the work presented in this dissertation. The challenges listed
% above, while fundamental to the problem domain can be practically alleviated in a variety of ways by
% exploiting modern network hardware. We provide evidence for the thesis statement above by addressing
% the following two questions.
% %%
% \textit{Where and how should serialization occur?} The default answer to these questions is "on the
% NIC" and "with RDMA atomic verbs." However, given the landscape of programmable in-network hardware,
% our options are flexible. At rack-scale, both TORs and NICs offer serialization points. The
% interface and mechanism for serialization can be customized using programmable hardware. For
% instance, a switch could be programmed to maintain locks~\cite{netlock}, provide
% sequencing~\cite{when-computer}, or directly implement contended functions~\cite{mind}.
% Simultaneously, NICs, though lower bandwidth and less centralized than TORs, have the potential to
% offer extended RDMA interfaces~\cite{prism} and implement OS functionality for remote
% clients~\cite{clio, supernic}.
% %%
% \textit{What data structures should be used?} Few data structures are currently designed for remote
% memory~\cite{clover,fusee,race,sherman,rolex,ditto}. While these systems resemble
% RDMA~\cite{pilaf,herd,cell} and NUMA~\cite{flat-combining,hopscotch,bbn} systems of the past,
% disaggregation requires special consideration for the network hardware it runs on. How can round
% trips and access latencies be reduced? What data structures are easy to build, and which are hard?
% These questions are a key focus of this dissertation.

% This dissertation explores the design of shared data structures in disaggregated memory systems. I
% introduce two systems, {\sword} and RCuckoo, which address the challenges of sharing and contending
% access to remote memory. {\sword} (Chapter~\ref{chap:swordbox}) adopts a middlebox approach to
% alleviate contention in shared data structures. Its key insight leverages the serialized view of
% traffic at rack-scale TORs, caching data structure state on a programmable switch. Additionally, I
% present RCuckoo (Chapter~\ref{chap:rcuckoo}), a fully disaggregated key-value store specifically
% designed to enhance key-value locality. RCuckoo utilizes locality-sensitive hashing to improve
% performance in reads, writes, locking, and fault recovery by minimizing round trips. {\sword}
% demonstrates significant improvements in both throughput (up to 30x) and tail latency (up to 300x),
% while RCuckoo meets or surpasses the performance of all state-of-the-art disaggregated key-value
% stores on small key-value pairs and outperforms other systems on the most common data-center
% workloads~\cite{ycsb,facebook-memcached}.
\end{dissertationintroduction}



\chapter{Forward Pass: On the Security Implications of Email Forwarding Mechanism and Policy}
\label{chap:swordbox}
\input{papers/email/1.intro.tex}
\input{papers/email/2.background.tex}
\input{papers/email/3.forward_in_the_wild.tex}
\input{papers/email/4.assumptions.tex}
\input{papers/email/5.attacks.tex}
\input{papers/email/6.disclosure.tex}
\input{papers/email/7.mitigation_and_discussion.tex}
\input{papers/email/8.conclusion.tex}



% \section{The Cost of Programmable Switches}

% As shown throughout its evaluation, \sword\ offers significant performance improvements over
% existing end-host solutions. The key insight behind \sword\ is that a small amount of
% programmability in the network combined with a massive amount of bandwidth can offer
% order-of-magnitude performance improvements for existing data structures, both lock-based and
% optimistic. However, \sword\ comes at a non-trivial cost. Programmable switches are expensive,
% complex, and difficult to program. While \sword\ offers a solution to the problem of contention, it
% is likely that only the most performance-critical applications would be likely to qualify for the
% care and attention required for crafting a \sword-like solution. Further complicating the matter is
% the fact that the Tofino series of switches has been discontinued by Intel~\cite{tofino-cancelled}.

% At a data structure level, the memory limitations of a switch pose a significant challenge for
% generalization. In the case of append operations made to a linked list, only the final value of the
% list needs to be cached to ensure the data structure's integrity. However, inserting into an
% arbitrary location in the list would require the entire list to be stored in cache. Given these
% operational complexities, high cost of development, and data structure limitations, we ask the
% question: \textit{What other techniques can be used to improve the performance of disaggregated data
% structures?}

% \section{Acknowledgement to {\sword} Contributors}

% Chapter~\ref{chap:swordbox} is a partial reprint of work submitted to multiple USENIX and ACM
% conferences under the title "SwordBox: Accelerating Shared Access in RDMA-based Disaggregated
% Memory. Stewart Grant, Alex C. Snoeren. This dissertations author was the primary investigator and
% author of this paper.

% Thank you to Alex C. Snoeren for his guidance and support throughout this project. Thank you to
% Rajdeep Das for his expertise in P4 and for supplying the initial P4 code and compiler configuration
% for our switch. Thank you to Anil Yelam for reviewing the figures and providing feedback on the
% initial draft of this work. Thank you to the reviewers at the \textit{Workshop on Resource
% Disaggregation and Serverless (WORDS '21)} for your feedback and revision notes on this work early
% in its development. Thank you to Geoffrey M. Voelker and Yiying Zhang for your feedback on this
% project.



\chapter{Disaggregated Data Structure Design}
\label{chap:rcuckoo}


\sword\ takes the stance that an additional piece of in-network equipment can be used to accelerate
an existing data structure. But what if we could just make the data structure itself faster? In this
chapter, we explore the design of a disaggregated key-value store, RCuckoo, which aims to answer
exactly this question. Instead of adding a new piece of equipment to accelerate a data structure,
with RCuckoo we aim to get better performance by co-designing itself with the network.

There are a variety of data structure-specific optimizations that have been leveraged to improve the
performance of disaggregated data structures.  Our position in this work is that locality-based
optimizations can provide significant benefit due to the high cost of round trips to remote memory
and the fact that network capacity continues to grow at an astonishing rate. In this chapter, we
make the case for locality-optimized cuckoo hashing and show how it can be used to improve
performance, reduce contention, and make use of the latest trends in network hardware.


% \input{rcuckoo/intro}
% \input{rcuckoo/background}
% \input{rcuckoo/body}
% \input{problems}
% \input{rcuckoo/design}
% \input{rcuckoo/evaluation}
%\input{limitation}
% \input{rcuckoo/conclusion}

% \chapter{Black Box Disaggregation}

% \section{Overview}
% \todo{finish up you writeup for a fall submission of BBD and paste it her}

\section{The Advantage of Locality}

The key insight behind RCuckoo is that locality can be used to improve it's performance. Each aspect
of it's design relies on it. Without locality spanning reads would span arbitrary ranges and consume
unacceptable amounts of bandwidth. Lock acquisition, without locality, would involve random
iterative reads throughout the table leading to many round trips and potential deadlock. Without
locality the lock table may not easily fit into a linear block of NIC memory. And finally without
locality the number of locks required for insertions would increase dramatically as the probability
of a single lock spanning multiple relevant entries would significantly decrease. Locality enables
us to take advantage of the fact that the network bandwidth is high and RDMA operations on linear
regions of memory.

In general RCuckoo is a demonstration of the power of locality in disaggregated systems, but it is
far from the final word. While many aspects of it's design are specific to Cuckoo hashing the
concept of reducing the number of round trips to perform an operation by increasing the likelihood
that all relevant information lies within a given range is general property we expect would provide
benefit many data structures.

\section{Acknowledgement to RCuckoo Contributors}

% Chapter~\ref{chap:rcuckoo} 
is a partial reprint of work submitted to multiple USENIX conferences
under the title "Cuckoo for Clients: Disaggregated Cuckoo Hashing. Stewart Grant, Alex C. Snoeren.
This dissertations author was the primary investigator and author of this paper.

% Thank you to Alex C. Snoeren for your tireless guidance and support throughout this work. Thank you
% to Dave Andersen for supplying an open source implementation of MemC3~\cite{memc3}, which was used
% as a reference for the search algorithm for RCuckoo. Thank you to Jiacheng Shen for our
% conversations at SOSP '23; without your confirmation that acquiring locks for a cuckoo hash
% \textit{was indeed hard} and that the progress we had made on the problem seemed promising, this
% work would not have been possible. Again, thank you to Anil Yelam for always providing feedback on
% the technical aspects of this project. Finally, thank you to Geoffrey M. Voelker for providing
% feedback on the initial draft of this work.


\chapter{Conclusion}

This dissertation explores the problems and solutions for sharing disaggregated memory. The high
access latency and extreme cost of contention in far-memory over RDMA cause many data structures,
which would otherwise be efficient, to experience performance collapse. Stale caches cause
opportunistic data structures to fail under contention, and pointer-based data structures incur
additional round trips when far-memory pointers need to be resolved. Using existing techniques,
applications can be made to work, but they simply have to incur the penalties of sharing when under
contention.

% We have presented two systems, {\sword} and RCuckoo, that present two different strategies for
% improving the performance of disaggregated data structures. {\sword} uses a programmable
% switch as a centralized cache to remove contention from shared data structures and to accelerate the
% use of locks on lock-based structures. RCuckoo uses locality within a hash table to improve the
% performance of reads and greatly reduce the cost of acquiring locks stored on a NIC. Together these
% works provide strong evidence that:


\begin{center}
\textit{Data structures can be optimized for disaggregated memory by leveraging network programmability} \\
\end{center}


In this chapter we begin by summarizing the contributions of this dissertation and their
relationship to our thesis claim, and we conclude with a discussion of this works limitations paired
with future work directions in this area of research.

\section{Contributions}

In this dissertation, we have contributed to the state of the art in disaggregated memory systems by:

\begin{itemize}
    \item Demonstrating the significant performance gains achievable by using a programmable switch to cache the contended state of a data structure and resolve the conflicts in the network.
    \item Showing that a programmable switch can reduce the instruction-level bottlenecks of RDMA atomic operations.
    \item Demonstrating that through locality optimizations locks can be fit into a small amount of NIC memory.
    \item Showing that RDMA-verbs are well suited for locality optimized data structures.
\end{itemize}

We believe that these contributions are significant stepping stones towards the design of future
efficient disaggregated systems. In the case of {\sword}, which demonstrated acceleration on
list appends, it could be adapted to other data structures with similar properties, for example,
log-structured systems. We believe that the insight behind RCuckoo's locality-based optimizations is
general and that many data structures could benefit from localizing their data in a similar fashion.

\section{Future Work}


Each of the works presented in this dissertation is a step towards more efficient disaggregated
systems. In this section, we speculate on future directions for this area of research based on the
limitations of the work presented here.

Many off-the-shelf ARM-based or FPGA-based SmartNICs could be used to implement {\sword} as well as
more complex data structures. While SmartNICs lack a global view of a rack, they have a global view
of the machine they are attached to and could be used to implement similar caching strategies.
SmartNICs typically have much more available memory than programmable switches and could likely
handle much larger data structures than {\sword}. For example, inserting into a linked list (because
the entire linked list needs to be stored in memory) is a \textit{difficult} data structure for
{\sword} to handle. This is unlikely to be the same on a SmartNIC with a few GB of memory. SmartNICs
could be used to cache index structures for large structures like B-Trees and use their limited
compute power to steer read and write requests similar to {\sword}. More generically, they could be
used for simple functions that are difficult to implement in remote memory, such as an allocator or
scheduler that needs to maintain centralized state.

% A further and more generic option for NIC designers is to extend the interface for RDMA to better
% accommodate complex one-sided data structures. While calls for pointer chasing are common, calls for
% more complex atomic operations are less so~\cite{prism, clio}. The algorithms community has designed
% many wait-free and lockless data structures such as binary trees and heaps that make use of
% multi-CAS. The ability to CAS multiple addresses simultaneously could open up disaggregation to many
% pointer-based data structures that would currently be difficult to implement with a single CAS, such
% as any structure which requires multiple pointers to be updated atomically, like a doubly linked
% list. Simultaneously CAS and FAA are of limited width. Extensions for larger CAS sizes would enable
% more efficient data structures, such as in the case of RACE and FUSEE, which must limit the size of
% their key-value pairs due to the lack of space in the 64-bit RDMA CAS.

% Network-data structure co-design is a powerful but underexplored area. In the case of
% {\sword}, Clover was a good fit for the system as only the tail pointer of linked lists needed
% to be stored on the switch. Other data structures do not always exhibit this property. Log-based or
% append-only data structures have promise here, as the point of contention (the end of the log) can
% be managed with relatively little state. Future work could enable \textit{append-mostly} data
% structures. Sherman, for instance, enjoys a degree of associativity at its leaf nodes by having
% associative leaves. Other data structures may be able to take advantage of similar properties, like
% appending updates to a shared log and eventually combining them into a consolidated data structure.

% Most proposals for disaggregated systems are focused on rack-scale deployments. As intra-rack
% latencies get lower and lower, and disaggregated technology gets better, intra-rack solutions will
% become more tenable. An early example is \textit{Disaggregating Stateful Network
% Functions}~\cite{dsnf} and \textit{SuperNIC}~\cite{supernic}, which take the stance that a large
% pool of dedicated accelerators can provide a significant portion of the network functions for a data
% center. In this line of work, routing is paramount and underexplored. {\sword} assumes a
% centralized model as it needs to track an entire data structure. However, a future distributed model
% could potentially scale to multiple racks if the data-dependent operations were routed through the
% correct network components.

The future for disaggregated systems is bright. This work has demonstrated that shared remote memory
can be made efficient with programmable networking hardware and that, through careful design, data
structures can be adapted to disaggregated memory. Hopefully, future work will build on these
techniques and enable a shift towards mainstream disaggregated computing.




% \chapter{ Example Figures and such for formatting reference}
% This demonstrates how OGS wants figures and tables formatted. For
% figures, the caption goes below the figure and ``Figure'' is in bold.
% See Figure~\ref{fig:zen}. Tables are formatted with the caption above
% the table. See Table~\ref{tab:bad}.

% Of course, Table~\ref{tab:bad} looks horrible. It should probably be
% formatted like Table~\ref{tab:good} instead.

% For facing caption pages, see Table~\ref{tab:facing}. Of course,
% facing caption pages are vaguely ridiculous and my implementation of
% them in the class file is by far the most brittle part of the
% implementation. It's entirely possible that something has changed and
% these don't work at all. I implemented it merely for the challenge.

% \begin{figure}
% \centering
% \fbox{\parbox{.9\linewidth}{%
% 	\noindent
% 	{\Huge PHD ZEN}\par
% 	\vskip.5in
% 	\centerline{comic here}
% 	\vskip.5in
% }}
% \caption[``Ph.D. Zen'']{Comic entitled ``Ph.D. Zen'' by Jorge Cham, 2005. Copyright
% has not been obtained and so it isn't displayed.}
% \label{fig:zen}
% \end{figure}

% \begin{table}
% \centering
% \caption[Electronic Dissertation Submission Rates]{Electronic
% Dissertation Submission Rates at UCSD, Fall 2005 and Winter 2006.
% (First two quarters that the program was available to all Ph.D.
% candidates not in a Joint Doctoral Program with SDSU.)}
% \label{tab:bad}
% \begin{tabular}{|*{5}{>{\centering\arraybackslash}m{.15\linewidth}|}}
% \hline
% &Ph.D.s awarded (Including Joint degrees) & Electronic submission of
% Dissertation & Paper Submission of Dissertation & Percentage of
% Electronic Submission\\
% \hline
% Fall\par 2005 & 84 & 37 & 47 & 44.05\%\\
% \hline
% Winter\par 2006 & 64 & 42 & 22 & 65.63\%\\
% \hline
% \end{tabular}
% \end{table}

% \begin{table}
% \centering
% \caption[Electronic Dissertation Submission Rates]{Electronic
% Dissertation Submission Rates at UCSD, Fall 2005 and Winter 2006.
% (First two quarters that the program was available to all Ph.D.
% candidates not in a Joint Doctoral Program with SDSU.)}
% \label{tab:good}
% \renewcommand\tabularxcolumn[1]{>{\RaggedRight\arraybackslash}p{#1}}
% \begin{tabularx}{.9\linewidth}{lcccc}
% \toprule
% &\multicolumn{1}{X}{Ph.D.s awarded (Including Joint degrees)}
% &\multicolumn{1}{X}{Electronic submission of Dissertation}
% &\multicolumn{1}{X}{Paper Submission of Dissertation}
% &\multicolumn{1}{X}{Percentage of Electronic Submission}\\
% \midrule
% Fall 2005 & 84 & 37 & 47 & 44.05\%\\
% Winter 2006 & 64 & 42 & 22 & 65.63\%\\
% \bottomrule
% \end{tabularx}
% \end{table}

% \begin{facingcaption}{table}
% \caption[UCSD Gender Distribution]{University of
% California, San Diego Gender Distribution for the Campus Population,
% October~2005\\
% (http://assp.ucsd.edu/analytical/Campus\%20Population.shtml)\\
% \emph{(This is an example of a facing caption page, the next page is
% the example of the table/figure/etc.\ that corresponds to this
% caption. It is also an example of table/figure that is rotated 90
% degrees to fit the page.)}}
% \label{tab:facing}
% \renewcommand\tabularxcolumn[1
% \setbox0=\vbox{%
% \vsize\textwidth
% \hsize\textheight
% \linewidth\hsize
% \columnwidth\hsize
% \textwidth\hsize
% \textheight\vsize
% \begin{tabularx}{\linewidth}{lXXXXXX}
% \toprule
% & \multicolumn{2}{c}{\textbf{Women}}
% & \multicolumn{2}{c}{\textbf{Men}}
% & \multicolumn{2}{c}{\textbf{Total}}\\
% \cmidrule(l){2-3}\cmidrule(l){4-5}\cmidrule(l){6-7}
% \textbf{Population Segment}
% & \multicolumn{1}{c}{\textbf{N}} & \multicolumn{1}{c}{\textbf{\%}}
% & \multicolumn{1}{c}{\textbf{N}} & \multicolumn{1}{c}{\textbf{\%}}
% & \multicolumn{1}{c}{\textbf{N}} & \multicolumn{1}{c}{\textbf{\%}}\\
% \midrule
% Students & 12,987 & 51\% & 12,686 & 49\% & 25,673 & 100\%\\
% Employees & 9,943 & 56\% &  7,671 & 44\% & 17,614 & 100\%\\
% \addlinespace
% \hfill\textbf{Total} & \textbf{22,930} & \textbf{53\%} &
% \textbf{20,357} & \textbf{47\%} & \textbf{43,287} & \textbf{100\%}\\
% \bottomrule
% \end{tabularx}
% \singlespacing

% \emph{Notes}:
% \begin{enumerate}
% \item The counts shown below will differ from the official quarterly
% Registrar's registration report because 1) data for residents in the
% Schools of Medicine and Pharmacy and Pharmaceutical Science are
% excluded, and 2) registered, non-matriculated, visiting students are
% included.
% \item Student workers are excluded from employees; however emeritus
% faculty and others on recall status are included.
% \end{enumerate}

% Campus Planning. Analytical Studies and Space Planning\\
% 31 January 2006
% }
% \centerline{\rotatebox{90}{\box0}}
% \end{facingcaption}

% % This will give us some more text
% % \Blinddocument

% % Skipping a bunch of chapters
% \begin{figure}
% \centering
% \fbox{\hbox to.8\linewidth{\hss Another figure\hss}}
% \caption{Another figure caption}
% \end{figure}
% \begin{table}
% \centering
% \caption{Another table caption}
% \begin{tabular}{ccc}
% \toprule
% X&Y&Z\\
% \midrule
% a&b&c\\
% \bottomrule
% \end{tabular}
% \end{table}
% \begin{figure}
% \caption{ASDF fig}
% \end{figure}
% \begin{table}
% \caption{ASDF tab}
% \end{table}

\appendix
% \Blinddocument
\bibliographystyle{plain} % Or whatever style you want like plainnat

\bibliography{thesis}

% Stuff at the end of the dissertation goes in the back matter
\backmatter

\end{document}


%%% Catch place for programmable switch use cases -- do not use
% \subsection{Programmable Switch Use (and Misuse) Cases}
% %%
% \todo{I have received a lot of push back on this section probably just cut it, I tried to put it in
% for alex but that was a bad idea.}

% Memory constraints on programmable network devices, as well as their programming model and
% architecture have a large impact on what kinds of systems can and should be built upon
% them~\cite{when-computer}. Cache size and pipeline depth have a large impact on what can be
% effectively stored on a programable switch. As part of our exploration of {\sword}
% (Chapter~\ref{chap:swordbox}) we examined the use of a programmable switch as a cache for the
% replicated log, and found that the switch for a variety of subtle reasons was not a good fit for our
% cache despite many prior works on in-network caching and key-value
% stores~\cite{netcache,netchain,netkv,netlock}.

% In the log system all clients must read all log entries for consistency. This read patten leads to a
% bandwidth bottleneck on the server hosting the log as the number of clients increases and the size
% of the log entry increases.  Figure~\ref{fig:bandwidth-model} Illustrates how as the number of
% clients and the size of individual log entries increases performance falls over. In our initial
% design we debated how to deal with the bottleneck. On one hand we could replicate the log across
% multiple memory servers and alleviate the broadcast bottleneck by spreading it across replicas.
% Alternatively we could cache the most recent writes on a programmable switch.  Initially we thought
% that the switch would be an excellent place for the log cache because we could easily reason about
% the access pattern of the log, and each client's link bandwidth would be proportional to how much
% they read. But the devil is in the details. Due to RDMA and switch pipeline constraints as described
% in the following section we found that the switch was an untenable solution.

% \begin{figure}
% \centering
% \includegraphics[width=0.99\linewidth]{fig/bandwidth-model.pdf}
% \caption{Bandwidth model for memory replication. 1MS is a single memory node, while 3MS is 3.}
% \label{fig:bandwidth-model}
% \end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.99\textwidth]{fig/switch-cache-model.pdf}
%     \caption{Performance improvement of caching on a programable switch as a function of hit rate and object cache size}
%     \label{fig:switch-cache-model}
% \end{figure}

% The values in Figure~\ref{fig:switch-cache-model} assume 64 clients and a single memory server and a
% log entry size of 8 bytes. The 0\% hit rate on cache sizes of 1024 is almost identical to the
% furthest left point in Figure~\ref{fig:bandwidth-model} with 64 clients and a single memory node.
% Note that with this cache size and a hit rate of 95\% which may be attainable considering the log
% access patterns are extremely predictable we are able to handle around 200 Million ops per second.

% Unfortunately current programmable switch hardware does not allow for cashable objects over 192
% bytes. Our Tofino 2 switch has a fixed sized 64 bit maximum storage capacity at each stage of it's
% pipeline, with a total of 24 stages counting both ingress and egress~\cite{tofino2}. Given a maximum
% cache size of 192 we can again calculate our maximum performance improvement. Using max sized RDMA
% reads of 192 bytes we significantly drop performance without the use of a switch, and in the common
% case get around 68 MOPS, our model without a switch striping across 3 memory servers yields a
% maximum of 57 MOPS. While still a theoretical advantage we consider the complexity of adding a
% programmable switch and caching for and 18\% benefits to be not worth it, especially considering
% that next generation NICs are slated for bandwidths of 800GBPS+. If future switch technology were to
% allow for larger object caches we believe this direction may be fruitful.

% \todo{Alex - put this insite somewhere in the intro}
% Given these kinds of subtle constraints the use of programmable network hardware must be carefully
% considered. In the case of {\sword} we explore the use of a programmable switch for caching parts of a
% shared data structure including locks, and parts of index structures. We found that some data
% structures, specifically those that require very little state in their index for a read or a write
% to succeed are well suited for caching. An example of this is appending to a linked list where only
% the last element of the list needs to be cached in order for an append to succeed. Alternatively
% inserting into an arbitrary location in a linked list is hard as all the nodes in the linked list
% need to be cached.



% \section{CXL}

% RDMA is not the only proposal for disaggregated memory. The Compute Express Link (CXL) is a new
% interconnect and standard supported by chip manufacturers and hyperscalers~\cite{cxl}. CXL is built on PCIe and
% aims to expand CPU memory capacity by allowing for the attachment of remote memory devices. Despite
% only a few CXL devices being available at the time of writing the technology is promising. Initial
% work has shown that CXL latencies for remote memory devices are between 2-5x the latency of local
% memory demonstrating that CXL devices can provide lower latency and higher throughput than
% RDMA~\cite{cxl-demyst,tpp,pond}. Given these promising results we expect that CXL will be a strong
% contender for remote memory pools.

% CXL has limitations that are not present in RDMA, specifically surrounding how memory is shared. CXL
% provides cache coherent memory semantics, however the expected cost of coherence similar to NUMA
% coherence will be high. Second CLX does not currently support a routing layer which limits it to the
% scalability of a CXL switch (likely rack scale). Finally CXL does not, at the time of writing,
% support in-network programmability. As we will show in our work in Chapter~\ref{chap:swordbox}
% in-network programmability is a powerful tool for reducing contention in shared data structures, the
% lack of any programmable devices in network may limit the ability to effectively share CXL devices.

% \todo{Alex - non sequitur}
% Prior work on remote memory data structures indicated that allowing CPU cores to subscribe to
% \textit{notify} groups could reduce the price of coherence while enabling fast shared
% structures~\cite{aguilera2019designing}. While notifications are not natively supported by CXL, CXL
% devices could be designed to support them without the need to build CXL programmable network
% devices. 

% \todo{Alex - Move this elsewhere}
% Alternatively techniques for NUMA scalability may also apply to remote memory such as black box
% approaches for NUMA data structures~\cite{bbn,nros,ironsync}. These techniques are powerful and
% enable general data structures to be built on NUMA architectures with low latency. While powerful
% these systems require each NUMA domain to synchronize its log with a singular log master, it also
% requires efficient support for atomic operations like CAS. It's unclear at this point in time if
% these approaches will scale up to the expected number of NUMA domains in a disaggregated deployment
% due to the fact that their bandwidth requirements grow linearly with the number of domains.